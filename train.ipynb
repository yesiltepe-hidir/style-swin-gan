{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg_resources import require\n",
    "from sklearn import discriminant_analysis\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "from utils import *\n",
    "from dataset import get_data_loader\n",
    "\n",
    "from models.discriminator import Discriminator\n",
    "from models.generator import Generator\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data from the loader\n",
    "def get_sample(loader):\n",
    "    while True:\n",
    "        for batch in loader:\n",
    "            yield batch\n",
    "\n",
    "\n",
    "# Train Generator and Discriminator\n",
    "def train(loader, generator, discriminator, g_optim, d_optim, device, args):\n",
    "    '''\n",
    "    Training function of Generator and Discriminator. \n",
    "    \n",
    "    Arguments:\n",
    "        args:          Contains process information \n",
    "        loader:        Data Loader\n",
    "        generator:     Style-Swin Transformer Generator\n",
    "        discriminator: Conv-Based discriminator\n",
    "        g_optim:       Generator Optimizer\n",
    "        d_optim:       Discriminator Optimizer\n",
    "        device:        Training device\n",
    "        \n",
    "    '''\n",
    "    # Yield a batch of data\n",
    "    loader = get_sample(loader)\n",
    "\n",
    "    # Set the configuration of training\n",
    "    losses = {}\n",
    "    # Initialize gradient penalty\n",
    "    grad_pen_loss = torch.tensor(0.0, device=device)\n",
    "    # L2 loss \n",
    "    l2 = nn.MSELoss()\n",
    "    # Gradient clipping\n",
    "    gradient_clip = nn.utils.clip_grad_\n",
    "\n",
    "    for iters in range(args.n_iters):\n",
    "        print(\"iters: ##########################\")\n",
    "        print(iters)\n",
    "        # ------------------ Train Discriminator -------------------- #\n",
    "        generator.train()\n",
    "        # Get batch of images and put them to device\n",
    "        real_img = next(loader).to(device)\n",
    "        # Avoid Generator to be updated\n",
    "        adjust_gradient(generator, False)\n",
    "        # Permit only discriminator to be updated\n",
    "        adjust_gradient(discriminator, True)\n",
    "        \n",
    "        # Sample random noise from normal distribution\n",
    "        noise_dim = (args.batch, args.dim) # ~ initial channel 512\n",
    "        noise = torch.randn(noise_dim).to(device) # ~ maybe .cuda()?\n",
    "\n",
    "        # Generate Fake image from random noise\n",
    "        fake_img = generator(noise)\n",
    "        # Get discriminator performance on generated images\n",
    "        fake_pred = discriminator(fake_img)\n",
    "        # Get discriminator performance on real images\n",
    "        real_pred = discriminator(real_img)\n",
    "\n",
    "        # Calculate Discriminator Loss\n",
    "        d_loss = discriminator_loss(real_pred, fake_pred)\n",
    "\n",
    "        # Update discriminator\n",
    "        discriminator.zero_grad()\n",
    "        d_loss.backward()\n",
    "        gradient_clip(discriminator.parameters(), 5.0)\n",
    "        d_optim.step()\n",
    "\n",
    "        # Employ Gradient Penalty\n",
    "        if iters % args.d_reg_every == 0:\n",
    "            real_img.requires_grad = True\n",
    "            # Get the prediction on updated discriminator\n",
    "            real_pred = discriminator(real_img)\n",
    "            # Calculate the R1 loss: Gradient penalty\n",
    "            grad_pen_loss = gradient_penalty(real_pred, real_img)\n",
    "            \n",
    "            # Update Discriminator\n",
    "            discriminator.zero_grad()\n",
    "            (args.scaler * (args.r1 / 2 * grad_pen_loss * args.d_reg_every)).backward()\n",
    "            grad_pen_loss.backward()  # ~ Ideally add some weighting... to this loss\n",
    "            d_optim.step()\n",
    "\n",
    "        # Save the losses\n",
    "        losses['discriminator'] = d_loss        \n",
    "        losses['gradient_penalty'] = grad_pen_loss\n",
    "\n",
    "        \n",
    "        # ------------------ Train Generator -------------------- #\n",
    "        \n",
    "        # Avoid Discriminator to be updated\n",
    "        adjust_gradient(discriminator, False)\n",
    "        # Permit only generator to be updated\n",
    "        adjust_gradient(generator, True)\n",
    "\n",
    "        # Get the next batch of real images\n",
    "        real_img = next(loader).to(device)\n",
    "\n",
    "        # Sample random noise from normal distribution\n",
    "        noise_dim = (args.batch, args.dim) # ~ initial channel 512\n",
    "        noise = torch.randn(noise_dim).to(device) # ~ maybe .cuda()?\n",
    "\n",
    "        # Generate Fake image from random noise\n",
    "        fake_img = generator(noise)\n",
    "        # Get discriminator performance on generated images\n",
    "        fake_pred = discriminator(fake_img)\n",
    "        \n",
    "        # Calculate the Generator loss\n",
    "        g_loss = generator_loss(fake_pred) # Ideally, add weight\n",
    "\n",
    "        # Save the loss\n",
    "        losses['generator'] = g_loss\n",
    "\n",
    "        # Update Generator\n",
    "        generator.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optim.step()\n",
    "\n",
    "        # Log and Save\n",
    "        if iters % args.print_freq == 0:\n",
    "            visualize_loss = {\n",
    "                'd_loss': d_loss,\n",
    "                'g_loss': g_loss,\n",
    "                'grad_pen_loss': grad_pen_loss,\n",
    "            }\n",
    "            \n",
    "            wandb.log(visualize_loss, step=iters)\n",
    "            print('Iters: {iters}\\t ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "NameError: name 'parse_arguments' is not defined\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'args = parse_arguments()\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/home/finch/git/style-swin-gan/train.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/finch/git/style-swin-gan/train.ipynb#ch0000013?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_cell_magic(\u001b[39m'\u001b[39;49m\u001b[39mpython3\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39margs = parse_arguments()\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/gan/lib/python3.8/site-packages/IPython/core/interactiveshell.py:2347\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2345\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2346\u001b[0m     args \u001b[39m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2347\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2348\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/gan/lib/python3.8/site-packages/IPython/core/magics/script.py:153\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     line \u001b[39m=\u001b[39m script\n\u001b[0;32m--> 153\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshebang(line, cell)\n",
      "File \u001b[0;32m~/anaconda3/envs/gan/lib/python3.8/site-packages/IPython/core/magics/script.py:305\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mraise_error \u001b[39mand\u001b[39;00m p\u001b[39m.\u001b[39mreturncode \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    301\u001b[0m     \u001b[39m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[39m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[39m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     rc \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mreturncode \u001b[39mor\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m9\u001b[39m\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'args = parse_arguments()\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%python3\n",
    "args = parse_arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'     \n",
    "# Parse Arguments\n",
    "\n",
    "# WandB\n",
    "wandb.init(project=\"styleswin\", entity=\"metugan\")\n",
    "\n",
    "# Generator \n",
    "generator = Generator(\n",
    "                dim=args.dim,\n",
    "                style_dim=args.style_dim,\n",
    "                n_style_layers=args.n_style_layers,\n",
    "                n_heads=args.n_heads,\n",
    "                resolution=args.resolution,\n",
    "                attn_drop=args.attn_drop \n",
    "            ).to(device)\n",
    "generator_learning_rate = args.gen_lr\n",
    "generator_betas = (args.beta1 , args.beta2)\n",
    "g_optim = optim.Adam(generator.parameters(), \n",
    "                    lr=generator_learning_rate, \n",
    "                    betas=generator_betas)\n",
    "# Discriminator\n",
    "discriminator = Discriminator(\n",
    "                    n_activ_maps=args.n_activ_maps,\n",
    "                    n_channels=3,\n",
    "                    resolution=args.resolution\n",
    "                ).to(device)\n",
    "discriminator_learning_rate = args.disc_lr\n",
    "discriminator_betas = (args.beta1 , args.beta2)\n",
    "d_optim = optim.Adam(discriminator.parameters(), \n",
    "                    lr=discriminator_learning_rate, \n",
    "                    betas=discriminator_betas)\n",
    "\n",
    "# Get DataLoader\n",
    "datasetname = 'CIFAR-10'\n",
    "root = './'\n",
    "batch_size = args.batch_size\n",
    "loader = get_data_loader(datasetname, root, batch_size)\n",
    "train(loader=loader, generator=generator, discriminator=discriminator,\n",
    "    g_optim=g_optim, d_optim=d_optim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gan')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1574755903cdb34dd59c92caa17a10f4d65534a19f0435b7dc632f13bd5d95f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
